<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
          		 	
			</div><!-- /header-area -->
			<hr class="break"/>
			<div id="content-area" class="container-16">
				<div id="content" class="grid-12 push-4">
    		<h1 id="page-title" >Readings</h1>    					              				</div><!-- #page-headline -->
		<div id="node-514" class="node  node-page clear-block">
  <div class="content clear-block">
            <span class='print-link'></span><p><b> Weekly Readings (Subject to Change) </b></p>
<table bgcolor="#F1F1F1" bordercolor="black" border=2>
<tr>
<td width=50> 01/23/2019 (Introduction, Backprop, Shallow Vs Deep,  Policies) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>

<li>Reducing the dimensionality of data with neural networks by Hinton et. al. (2006)  <a href="http://www.cs.toronto.edu/~hinton/science.pdf">Here  </a> </li> 
<li> How to read a technical paper by Jason Eisner (2009) <a href="http://www.cs.jhu.edu/~jason/advice/how-to-read-a-paper.html"> [HTML] </a></li>
<li> Chapter 6 of Deep Learning Book by Goodfellow <a href="http://www.deeplearningbook.org/">Here </a> </li>
</ul>

</td>
</tr>
<tr>
<td bgcolor="FFFFEE"> <b> Optional</b>
<ul>
<li> Chapters 1-5 of Deep Learning Book by Goodfellow <a href="http://www.deeplearningbook.org/"> Here </a> </li>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 01/30/2019 (Convolutional Neural Networks, Vision (ImageNet) ) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>

<li> Chapter 9 (Convolutional Neural Networks) of Deep Learning Book by Goodfellow <a href="http://www.deeplearningbook.org/"> Here </a>

<li> <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"> AlexNet </a> </li> (Presenter: )
</ul>
</td>
</tr>
<tr>
<td bgcolor="#FFC2E0"><b> Paper Critique and Summary before Class I  (Read 1 out of the 3) 
<ol>
<li> VGGNet <a href="https://arxiv.org/pdf/1409.1556.pdf"> Here </a> </li>
<li> GoogleNet <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf"> Here </a> </li>
<li> ResNet <a href="https://arxiv.org/pdf/1512.03385.pdf">Here </a></li>

</ol>
</td>
</tr>



<tr>
<td bgcolor="FFFFEE"> <b> Optional</b> 
<ul>
<li> 
</li>

</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 02/06/2019 (Advanced Training) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> Dropout: a simple way to prevent neural networks from overfitting by Srivastava et. al. <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"> Here </a> 
<li> Chapters 7 and  8 of Deep Learning Book by Goodfellow <a href="http://www.deeplearningbook.org/"> Here </a>
<li> Net2net: Accelerating learning via knowledge transfer." arXiv preprint arXiv:1511.05641 (2015)  by Chen et. al. <a href="https://arxiv.org/abs/1511.05641"> Here </a> (Presenter: ) </li>
<li> Adam: A method for stochastic optimization by Diederik et. al. (2014) <a href="http://arxiv.org/pdf/1412.6980"> Here </a> (Presenter: ) </li>
</ul>
</td>
</tr>
<tr>
<td bgcolor="FFFFEE"> <b> Optional</b> 

<ul>
<li> 
Sec 2.2 of DL Papers Reading Roadmap: <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap#22-optimization"> Here </a> 

</li>
<li>
Sec 2.1 of DL Papers Reading Roadmap: <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap#21-model"> Here </a>
</li>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 02/13/2019 (Unsupervised Learning/Deep Generative Models) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> Generative Adverserial Networks by Goodfellow (2014) <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf"> Here </a> (Presenter: )</li>
<li> UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS  by Radford et. al. (2015)  (Presenter: ) <a href="http://arxiv.org/pdf/1511.06434"> Here </a> </li>
<li> 
</ul>
</td>
</tr>


<tr>
<td bgcolor="FFFFEE"> <b> Optional</b> 
<ul>
<li> Chapter 14 (Autoencoders)  of deep learning book <a href="http://www.deeplearningbook.org/contents/autoencoders.html"> Here </a> </li>
<li> Papers on Unsupervised/Generative learning <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap#23-unsupervised-learning--deep-generative-model"> Here </a> </li> 
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 02/20/2019 (Sequence Modeling) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> Chapter 10 of Deep Learning Book on Sequence Modeling <a href="http://www.deeplearningbook.org/contents/rnn.html"> Here </a> </li>
</ul>
</td>
</tr>


<td bgcolor="#FFC2E0"><b> Paper Critique and Summary before Class II  (Read 1 out of the 4) </b>
<ol>
<li> Generating sequences with recurrent neural networks." arXiv preprint arXiv:1308.0850 (2013).  by Graves  <a href="http://arxiv.org/pdf/1308.0850"> Here </a> </li>
<li>  Sequence to sequence learning with neural networks." Advances in neural information processing systems. 2014 by Sutskever (2014)<a href="https://arxiv.org/pdf/1409.3215.pdf"> Here </a> </li>
<li> Visualizing and Understanding Recurrent Neural Networks by Karpathy et. al. ICLR (2015)  <a href="https://arxiv.org/pdf/1506.02078.pdf">Here </a></li>
<li> PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs (NIPS 2017)	by Wang et. al. <a href="https://papers.nips.cc/paper/6689-predrnn-recurrent-neural-networks-for-predictive-learning-using-spatiotemporal-lstms.pdf"> Here </a></li>
<li> Gated Feedback Recurrent Neural Networks by Chung et. al. (2015)  <a href="https://arxiv.org/pdf/1502.02367v3.pdf"> Here </a> </li>
<tr>
<td bgcolor="FFFFEE"> <b> Optional</b>
<ul>
<li> Papers on Sequence Modeling <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap#24-rnn--sequence-to-sequence-model"> Here </a> </li>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 02/27/2019 (Applications: NLP )  </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> 
Distributed Representations of Words and Phrases and their Compositionality by Mikolov et. al. (2013) <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"> Here </a> 
</li>
</ul>
</td>
</tr>


<td bgcolor="#FFC2E0"><b> Paper Critique and Summary before Class III  (Read 1 out of the 4) </b>
<ol>
<li> Attention is All you Need by Vaswani et. al. (2017) <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"> Here </a></li> 
<li> Ask Me Anything:Dynamic Memory Networks for Natural Language Processing by Kumar et. al. (2016) <a href="https://arxiv.org/pdf/1506.07285.pdf"> Here </a></li>
<li> Global Vectors for Word Representation by Pennington et.  al. <a href="https://nlp.stanford.edu/pubs/glove.pdf"> Here </a> </li>
<li> Adversarial Multi-task Learning for Text Classification by Liu et. al. (2017) <a href="https://arxiv.org/pdf/1704.05742.pdf"> </a> </li>
<li> An Empirical Exploration of Recurrent Network Architectures by Jozefowicz et. al. 2015 <a href="http://proceedings.mlr.press/v37/jozefowicz15.pdf"> Here </a> </li>
</ol>
<tr>

<tr>
<td bgcolor="FFFFEE"> <b> Optional</b>
<ul>

<li>
Efficient Estimation of Word Representations in Vector Space <a href="https://arxiv.org/pdf/1301.3781.pdf"> Here </a> </li>
<li> See class by Richard Socher <a href="http://cs224d.stanford.edu/syllabus.html"> Here </a>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 03/06/2019 (Applications: Recommender Systems) </td>

<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> Deep Learning based Recommender System: A Survey and New Perspectives by Zhang et. al. (2018) <a href="https://arxiv.org/pdf/1707.07435.pdf"> Here </a>  (Presenter: )</li>
<li> Neural Collaborative Filtering by He et. al. (2017) <a href="https://arxiv.org/pdf/1708.05031.pdf"> Here </a> (Presenter: ) </li>
</ul>
</td>
</tr>
<tr>
<td bgcolor="FFFFEE"> <b> Optional</b> 
<ul>
<li> Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention by Chen et. al. 2017 <a href="https://dl.acm.org/citation.cfm?id=3080797"> Here </a> </li>
</ul>

</td>

</tr>
</table>
</td></tr>
<tr>
<td width=50> 03/20/2019 (Deep Learning in X) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> Opportunities and obstacles for deep learning in biology and medicine by Ching et. al. (2018) <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0387"> Here </a> (Presenter: )</li>
<li> A Neural Algorithm of Artistic Style by Gatys et. al. (2015) <a href="https://arxiv.org/pdf/1508.06576.pdf"> Here </a> (Presenter: )  </li>
<li> Deep Learning in Medical Image Analysis by Shen et. al. (2017) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722/">Here </a> (Presenter: ) </li>
</ul>
</td>
</tr>
<tr>
<td bgcolor="FFFFEE"> <b> Optional</b> 
<ul>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 03/27/2019 (Deep Reinforcement Learning) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li>Human-level control through deep reinforcement learning by Mnih et. al. (2015) <a href="https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf"> Here </a></li>
<li>Dueling network architectures for deep reinforcement learning by Ziyu et. al. (2015) <a href="http://arxiv.org/pdf/1511.06581"> Here </a> </li>
<li>Mastering the game of Go with deep neural networks and tree search." Nature 529.7587 (2016): 484-489  <a href="http://willamette.edu/~levenick/cs448/goNature.pdf"> Here </a>
</ul>
</td>
</tr>
<tr>
<td bgcolor="FFFFEE"> <b> Optional</b>
<ul>
<li>Papers on DL <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap#26-deep-reinforcement-learning"> here </a>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 04/03/2019 (Transfer/One-Shot DL) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> Progressive Neural Networks by Rusu et. al. (2016) <a href="https://arxiv.org/pdf/1606.04671.pdf"> Here </a>  (Presenter: )</li>
<li> Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (2016) by Xu. et. al. <a href="https://arxiv.org/pdf/1502.03044v3.pdf"> Here </a> (Presenter: ) </li>
<li> How Transferable are features in deep neural networks <a href="https://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf"> Here </a> </li>
</ul>
</td>
</tr>
<tr>
<td bgcolor="FFFFEE"> <b> Optional</b> 
<ul>
<li> Papers from DL <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap#27-deep-transfer-learning--lifelong-learning--especially-for-rl"> here </a>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 04/10/2019 (Resource-Aware Deep Learning) </td>
<td>
<table>
<tr>
<td> <b> Required</b>  
<ul>
<li> Deep Learning for IoT Big Data and Streaming Analytics: A Survey by Mohammadi et. al. <a href="https://arxiv.org/pdf/1712.04301.pdf"> Here </a> </li>
</ul>
</td>
</tr>

<tr>
<td bgcolor="#FFC2E0"><b> Paper Critique and Summary before Class IV  (Read 1 out of the 4) </b> 

<ol>
<li> Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding by Song et. al. (2015)  <a href="https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf"> Here </a>
<li> SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and less than 1MB model size. arXiv preprint arXiv:1602.07360 by Forrest et. al.  (2016).  <a href="http://arxiv.org/pdf/1602.07360"> Here </a></li>
<li> Deep Learning for the Internet of Things by Yao et. al. (2018) <a href="https://cse.buffalo.edu/~lusu/papers/Computer2018.pdf"> Here </a>
<li> Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1 by Matthieu et. al. <a href="https://arxiv.org/pdf/1602.02830.pdf"> Here </a> </li>

</ol>
</td>
</tr>



<tr>
<td bgcolor="FFFFEE"> <b> Optional</b> 
<ul>
<li> Papers by Yao (UIUC) <a href="https://yscacaca.github.io/"> Here </a>
</ul>
</td>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 04/17/2019 (Free/Work on Projects) </td>
</td></tr>
<tr>
<td width=50> 04/24/2019 (Project Presentations I) </td>
<td>
<table>
<tr>
</tr>
</table>
</td></tr>
<tr>
<td width=50> 05/01/2019 (Project Presentations II) </td>
<td>
<table>
<tr>
</td>
</tr>
</table>


</html>
